["MATERIALS AND METHODS", "Obtaining fungal and bacterial BGC-SM pairs with known bioactivities", "The fungal BGC GenBank files of 392 fungal BGCs were downloaded from the MIBiG database, version 3.0 (24). We performed a literature search of bioactivity and growth assays to identify validated bioactivities of the SM products from all 392 fungal BGCs in MIBiG. Data on the 1,152 bacterial BGCs and their SM product bioactivities were retrieved from the study by Walker and Clardy (30). SM bioactivities were categorized into \u201cantibacterial,\u201d \u201cantifungal,\u201d \u201ccytotoxic,\u201d \u201cantitumor,\u201d and \u201cunknown\u201d for when data were not available. We also identified SM bioactivities that were not included in our predictions (e.g., antifeedant) because of their small numbers. Bioactivity classifications were converted into binary matrices, such that \u201c1\u201d indicated the presence of activity and \u201c0\u201d indicated the absence of activity or lack of documentation. These data are available in Tables S1 and S2 (https://doi.org/10.6084/m9.figshare.24129012).", "Feature selection and construction of training data sets", "Our training data included the BGC number (corresponding to the accession number of the BGC in the MIBiG database), product name, the SM product bioactivity, core gene present in the BGC that biosynthesizes the backbone of the SM product, and the species and genus classification. BGCs whose bioactivities are unknown were not included in model training. Two training data sets were used: the first contained fungal BGCs and the bioactivities of their corresponding SMs (392 BGCs in total; the SMs of 123 BGCs had antibacterial bioactivity, of which 115 had antifungal bioactivity, 96 had cytotoxic bioactivity, and 123 had antitumor bioactivity), and the second contained both fungal (392 BGCs) and bacterial BGCs (1,544 BGCs in total; the SMs of 627 BGCs had antibacterial bioactivity, of which 312 had antifungal bioactivity, 328 had cytotoxic bioactivity, and 260 had antitumor bioactivity) and the bioactivities of their SMs. From the 392 BGCs in the fungal data set, 78 were removed because their bioactivities were unknown, resulting in 314 BGCs used in training the models; similarly, from the 1,544 BGCs in the fungal and bacterial data set, 227 were removed because their bioactivities were unknown, leaving 1,317 BGCs (314 fungal and 1,003 bacterial) used in training the models.", "To obtain the features for model training, each GenBank file for the BGCs was run through anti-SMASH, version 5 (36). For each BGC, we extracted from the GenBank output files generated by anti-SMASH: (i) Pfam protein family domains represented in the BGC, (ii) the core gene(s) (PKS, NRPS, etc.), (iii) cluster-defining CDS features [gene features that anti-SMASH uses to define the BGC class or classes (e.g., NRPS, PKS, etc.)], and (iv) annotations of secondary metabolite clusters of orthologous groups of proteins (smCOGs, annotations for accessory genes in BGCs based on sequence similarity to genes in other characterized BGCs). Extractions were performed using Python scripts modified from Walker and Clardy (30) (https://doi.org/10.6084/m9.figshare.24129012). To identify genes in the BGCs similar to antibiotic resistance genes, which have the potential to be predictive of antibacterial and antifungal bioactivity, the GenBank files generated by anti-SMASH, one for each BGC, were converted into fasta files and subsequently run through Resistance Gene Identifier (RGI), (version 5) (37). The RGI annotations were extracted similarly to the anti-SMASH output with a Python script, retaining only the genes that occurred five or more times in the data set (https://doi.org/10.6084/m9.figshare.24129012). All the features collected were converted into binary matrices and used in model training. These data are available in Tables S6 and S7.", "Machine learning models", "To gain insights into fungal SM bioactivity, we trained machine learning models to predict three types of bioactivity: antibacterial, antifungal, and cytotoxic/antitumor (Fig. 1). Following Walker and Clardy (30), we used three algorithms to make the three binary classifications: (i) support vector clustering (SVC) module for SVM, (ii) stochastic gradient decent classifier (SGDClassifier) module for the LR, and (iii) RF with extra randomized decision trees. All three classifiers were used independently to predict each type of bioactivity since an SM can have multiple bioactivity types. Each classifier was imported from the scikit-learn Python library (38), and the parameters were determined by completing a GridSearch from scikit-learn.", "The parameter values used in the GridSearch for the SVM models were c-values of 100, 10, 1, 0.5, 0.1, and 0.01 and gamma values of 0.01, 0.1, 1, and 10. The c-value is the regularization parameter that determines the hyperplane (e.g., a high c-value will choose a smaller-margin hyperplane focused on classifying training data points correctly and can lead to overfitting, and a small c-value will choose a larger-margin hyperplane to find a generalized smooth boundary but can lead to underfitting). The gamma values determine the similarity radius; a small gamma value leads to a more general model, whereas a larger gamma leads to a model more specific to the training data.", "The parameter values used in the GridSearch for the LR models were maximum iterations of 100; log loss and elasticnet penalties; alpha values of 0.5, 0.3, 0.2, 0.1, 0.01, 0.001, 0.0001, 0.00001, and 0.000001; and l1 ratios of 0.5, 0.2, 0.05, 0.1, 0.01, 0.001, and 0.0001. Additionally, the tolerance was set to \u201cnone.\u201d The alpha value is a penalty score for large correlation coefficients, which prevents the models from overfitting (e.g., higher alpha values favor simpler models with lower correlation coefficients). The l1 ratios control the balance between lasso (some coefficients are exactly zero and few features are important) and ridge (keeps the coefficients from being too large and balances the coefficients of all features) regularization, such that underfitting and overfitting are prevented. Lastly, the tolerance parameter determines the point where the model stops training based on the differences in model performance from one iteration to the next.", "The parameter values for the RF models were maximum features on auto; Gini criterion; bootstraps true; maximum depth of 10, 20, 50, 100, 1,000, and none; and the number of estimators were 1, 5, 10, 15, 25, 50, and 100. The Gini criterion measures the degree of node impurity (the amount of variance in a given feature for BGCs that either have or do not have this feature) and is used to split nodes in each decision tree to reduce impurity. Maximum depth controls how deep the decision tree can grow to prevent overfitting, and the number of estimators is the number of decision trees in the forest. The best parameter values for each classifier were chosen based on the highest average accuracy.", "To evaluate the models, we used a 10-fold cross-validation (10 trials, each of which holds back a random 10% of the training data for testing) to calculate the balanced accuracy metric from the scikit-learn Python library. We used balanced accuracy to address the imbalance of the bioactivity types in the training data since the bioactivities are rare and therefore have a larger portion of the 0\u2019s compared to 1\u2019s [e.g., there are 116 BGCs with antifungal bioactivity (1\u2019s) compared to 198 without (0\u2019s) in our fungal BGC training data]. Balanced accuracy takes the mean of the true positive rate (TNR) and the true negative rate to provide a more accurate representation of model performance on imbalanced data sets. The balanced accuracy of each classifier was compared to classifiers trained on randomized features that represent the inability to distinguish between classes (0 or 1). A balanced accuracy score of 50% means that the classifier predicts the correct classification as well as randomly guessing.", "Each classifier for each classification was assessed using a one-way ANOVA to determine if there was a significant difference between using the actual training data vs the randomized data. All the ANOVAs were performed through the Python library SciPy (39) and assessed using alpha levels of 0.0001, 0.001, 0.01, and 0.05. Classifiers were also evaluated with receiver operator characteristic (ROC) curves and precision-recall (PR) curves. ROC curves plot the true positive rate against the false-positive rate (FPR). Classifiers with the largest area under the curve (AUC) values have better true-to-false-positive ratios, with AUC values greater than 0.5 indicating better than random ability to correctly predict the presence/absence of a given bioactivity. The PR curves plot the recall (i.e., true positives over the sum of true positives and false negatives) against the precision (i.e., true positives over the sum of the true positives and the false positives); they are considered more accurate for classifiers trained on imbalanced data sets, such as ours.", "Assembling the fungal phylogeny", "To analyze the distribution of the characterized SM bioactivities across the fungal kingdom we displayed the different types of bioactivity known for different species on the branch tips of a phylogeny using the Interactive Tree of Life (iToL) tool, version 6 (40). The phylogeny was modified with no tip labels and no branch lengths from a previous phylogenomic analysis of 290 genes from 1,644 fungal species (41). If a species in the data set was not present in the phylogeny, we chose a close relative in the same genus; if the genus was also absent, the data for that species were not displayed on the phylogeny."]